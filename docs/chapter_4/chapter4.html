

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 4: AI isn’t magic, but it’s okay if it feels like it &#8212; Data Analytics and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_4/chapter4';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 5: What makes good models good and bad models bad?" href="../chapter_5/chapter5.html" />
    <link rel="prev" title="Chapter 3: Charting the unknown: stocking your explorer’s toolkit" href="../chapter_3/chapter3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/Logo_UCLL_ENG_RGB.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/Logo_UCLL_ENG_RGB.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_1/chapter1.html">Chapter 1: What makes data science, machines learn, and intelligence artificial?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_2/chapter2.html">Chapter 2: Bringing the right equipment to start your data adventure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_3/chapter3.html">Chapter 3: Charting the unknown: stocking your explorer’s toolkit</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 4: AI isn’t magic, but it’s okay if it feels like it</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_5/chapter5.html">Chapter 5: What makes good models good and bad models bad?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ucll-daml/book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ucll-daml/book/issues/new?title=Issue%20on%20page%20%2Fchapter_4/chapter4.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_4/chapter4.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 4: AI isn’t magic, but it’s okay if it feels like it</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-intuition-to-algorithms">From Intuition to Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-basic-rules-capturing-relationships">Beyond Basic Rules: Capturing Relationships</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-vs-regression-predicting-categories-or-values">Classification vs. Regression: Predicting Categories or Values</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-predicting-categories">Classification: Predicting Categories</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-in-the-travel-agency-context">Examples in the Travel Agency Context:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-predicting-values">Regression: Predicting Values</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Examples in the Travel Agency Context:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-right-task">Choosing the Right Task</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-knn-intuition-and-implementation">K-Nearest Neighbors (KNN): Intuition and Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-similarity-distance-metrics">Defining Similarity: Distance Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-knn-algorithm-finding-the-neighbors">The KNN Algorithm: Finding the Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-value-of-k">Choosing the Value of k</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrative-examples-with-travel-agency-data">Illustrative Examples with Travel Agency Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#detour-preprocessing-preparing-your-data-for-supervised-learning">Detour: Preprocessing - Preparing Your Data for Supervised Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling-leveling-the-playing-field">Feature Scaling: Leveling the Playing Field</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-representing-categorical-variables">One-Hot Encoding: Representing Categorical Variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-trees-branching-paths-to-predictions">Decision Trees: Branching Paths to Predictions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-capturing-linear-relationships">Linear Regression: Capturing Linear Relationships</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-equation-of-a-line">The Equation of a Line:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-best-fitting-line-minimizing-the-error">Finding the Best-Fitting Line: Minimizing the Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-coefficients">Interpreting the Coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-predicting-probabilities">Logistic Regression: Predicting Probabilities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-function-mapping-values-to-probabilities">The Sigmoid Function: Mapping Values to Probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-for-binary-classification">Logistic Regression for Binary Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-coefficients">Interpreting Coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-the-power-of-predictive-modeling">Conclusion: The Power of Predictive Modeling</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_remove-input docutils container">
</div>
<section class="tex2jax_ignore mathjax_ignore" id="chapter-4-ai-isn-t-magic-but-it-s-okay-if-it-feels-like-it">
<h1>Chapter 4: AI isn’t magic, but it’s okay if it feels like it<a class="headerlink" href="#chapter-4-ai-isn-t-magic-but-it-s-okay-if-it-feels-like-it" title="Permalink to this heading">#</a></h1>
<p>We’ve journeyed through the landscape of our data, gaining valuable insights through exploration and visualization. Armed with this knowledge, we can make informed decisions about our travel agency’s operations, marketing strategies, and customer service. But as programmers, we crave efficiency and automation. Can’t we go a step further and teach machines to make these decisions for us?</p>
<section id="from-intuition-to-algorithms">
<h2>From Intuition to Algorithms<a class="headerlink" href="#from-intuition-to-algorithms" title="Permalink to this heading">#</a></h2>
<p>Imagine trying to automate decisions based on our data. We could write countless if-then-else statements to capture every possible scenario:</p>
<ul class="simple">
<li><p>“If the customer is young and traveling with friends, recommend adventurous destinations.”</p></li>
<li><p>“If the customer is older and prefers luxury, suggest a relaxing beach vacation.”</p></li>
<li><p>…</p></li>
</ul>
<p>But this quickly becomes overwhelming. What if we could extract these decision-making rules directly from the data itself? This is the essence of supervised learning.</p>
<p>The core intuition behind supervised learning is simple: similar inputs should lead to similar outputs. A young adventurer booking a trip to the mountains is likely to have similar preferences to another young adventurer. But how do we define “similar”? This leads us to algorithms like K-Nearest Neighbors (KNN), which classify new data points based on their proximity to known data points.</p>
<p>This is where the magic of AI starts to emerge. We’re not explicitly programming the rules; we’re allowing the algorithm to learn them from the data. Sometimes, the output we want is a simple yes/no (classification, like predicting whether a customer will book a trip). Other times, it’s a continuous value (regression, like predicting the price a customer is willing to pay).</p>
</section>
<section id="beyond-basic-rules-capturing-relationships">
<h2>Beyond Basic Rules: Capturing Relationships<a class="headerlink" href="#beyond-basic-rules-capturing-relationships" title="Permalink to this heading">#</a></h2>
<p>But we can be even smarter. Instead of just looking at similar inputs, what if we could learn a more general relationship between the input and output? This is where linear regression comes in. Remember those correlations we explored? Linear regression is like their sophisticated cousin, finding the best-fitting line to capture the relationship between two variables. It’s also related to the concept of variance, though the exact details of how these statistical concepts intertwine can remain a bit magical for now.</p>
<p>In this chapter, we’ll delve into these fundamental supervised learning algorithms, exploring how they work, how to train them, and how to evaluate their performance. We’ll also learn about essential techniques like feature scaling, which ensures that all our variables are on a level playing field. Get ready to unlock the power of predictive modeling and witness how AI can transform data into actionable insights.</p>
</section>
<section id="classification-vs-regression-predicting-categories-or-values">
<h2>Classification vs. Regression: Predicting Categories or Values<a class="headerlink" href="#classification-vs-regression-predicting-categories-or-values" title="Permalink to this heading">#</a></h2>
<p>In the realm of supervised learning, there are two primary types of tasks: classification and regression. The choice between them depends on the nature of the output you’re trying to predict.</p>
<section id="classification-predicting-categories">
<h3>Classification: Predicting Categories<a class="headerlink" href="#classification-predicting-categories" title="Permalink to this heading">#</a></h3>
<p>Classification tasks involve predicting a categorical or discrete output. The goal is to assign a data point to one of several predefined categories or classes. Think of it as sorting things into different buckets.</p>
<section id="examples-in-the-travel-agency-context">
<h4>Examples in the Travel Agency Context:<a class="headerlink" href="#examples-in-the-travel-agency-context" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Predicting whether a customer will book a trip (yes/no).</p></li>
<li><p>Classifying customer reviews as positive, negative, or neutral.</p></li>
<li><p>Predicting the package type a customer will choose (adventure, relaxation, or cultural).</p></li>
</ul>
</section>
</section>
<section id="regression-predicting-values">
<h3>Regression: Predicting Values<a class="headerlink" href="#regression-predicting-values" title="Permalink to this heading">#</a></h3>
<p>Regression tasks involve predicting a continuous or numerical output. The goal is to estimate a value for a given input. Think of it as drawing a line or curve that best fits the data.</p>
<section id="id1">
<h4>Examples in the Travel Agency Context:<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Predicting the cost of a trip.</p></li>
<li><p>Predicting the length of stay for a customer.</p></li>
<li><p>Predicting the number of guests for a booking.</p></li>
</ul>
</section>
</section>
<section id="choosing-the-right-task">
<h3>Choosing the Right Task<a class="headerlink" href="#choosing-the-right-task" title="Permalink to this heading">#</a></h3>
<p>The choice between classification and regression depends on the specific question you’re trying to answer, but essentially it’s very straightforward:</p>
<ul class="simple">
<li><p>If you want to predict a category or label, use classification.</p></li>
<li><p>If you want to predict a numerical value, use regression.</p></li>
</ul>
<p>Sometimes, you might need to transform your data or reframe your question to fit one of these tasks. For example, if you want to predict customer satisfaction on a scale of 1 to 10, you could treat it as a regression problem (predicting the numerical satisfaction score) or as a classification problem (grouping satisfaction scores into categories like “low,” “medium,” and “high”).</p>
<p>Understanding the distinction between classification and regression is crucial for selecting the appropriate algorithms and evaluation metrics for your supervised learning task.</p>
</section>
</section>
<section id="k-nearest-neighbors-knn-intuition-and-implementation">
<h2>K-Nearest Neighbors (KNN): Intuition and Implementation<a class="headerlink" href="#k-nearest-neighbors-knn-intuition-and-implementation" title="Permalink to this heading">#</a></h2>
<p>Let’s start with a common supervised learning technique that illustrates how this all works.</p>
<p>The core idea behind K-Nearest Neighbors (KNN) is remarkably simple yet surprisingly powerful. It’s based on the intuition that similar things tend to be located near each other. Think about your own experiences:</p>
<ul class="simple">
<li><p>Birds of a feather flock together.</p></li>
<li><p>You’re more likely to share similar interests with your close friends than with strangers.</p></li>
<li><p>Houses in the same neighborhood often have similar architectural styles and values.</p></li>
</ul>
<p>KNN applies this same principle to data. It assumes that data points with similar characteristics (features) will be located close to each other in the feature space. This allows us to make predictions about a new data point by looking at its “nearest neighbors” – the data points that are most similar to it.</p>
<section id="defining-similarity-distance-metrics">
<h3>Defining Similarity: Distance Metrics<a class="headerlink" href="#defining-similarity-distance-metrics" title="Permalink to this heading">#</a></h3>
<p>But how do we measure “similarity” between data points? KNN uses distance metrics to quantify the similarity between data points in the feature space. Some common distance metrics include:</p>
<ul class="simple">
<li><p>Euclidean Distance: The straight-line distance between two points (like the Pythagorean theorem).</p></li>
<li><p>Manhattan Distance: The distance between two points measured along axes at right angles (like walking city blocks).</p></li>
<li><p>Chebyshev distance: The maximum metric or L∞ metric - the maximum of the absolute differences between the corresponding coordinates of the two points for each variable.</p></li>
</ul>
<p>The choice of distance metric can influence the performance of the KNN algorithm, and the best choice often depends on the specific characteristics of the data.</p>
</section>
<section id="the-knn-algorithm-finding-the-neighbors">
<h3>The KNN Algorithm: Finding the Neighbors<a class="headerlink" href="#the-knn-algorithm-finding-the-neighbors" title="Permalink to this heading">#</a></h3>
<p>The KNN algorithm itself is straightforward:</p>
<ol class="arabic simple">
<li><p>Calculate Distances: Calculate the distance between the new data point and all other data points in the training set.</p></li>
<li><p>Find Nearest Neighbors: Identify the k data points that are closest (most similar) to the new data point. These are the “k-nearest neighbors.”</p></li>
<li><p>Make Predictions: KNN (and many supervised learning techniques) works for both classification and regression problems.</p></li>
</ol>
<ul class="simple">
<li><p>Classification: For classification tasks, assign the new data point to the class that is most frequent among its k-nearest neighbors (majority voting).</p></li>
<li><p>Regression: For regression tasks, predict the value of the new data point by averaging the values of its k-nearest neighbors.</p></li>
</ul>
<section id="choosing-the-value-of-k">
<h4>Choosing the Value of k<a class="headerlink" href="#choosing-the-value-of-k" title="Permalink to this heading">#</a></h4>
<p>The parameter k (the number of nearest neighbors to consider) is a crucial factor in the performance of the KNN algorithm. A small value of k can lead to overfitting (the model is too sensitive to noise in the data), while a large value of k can lead to underfitting (the model is too simplistic and misses important patterns). The optimal value of k often needs to be determined through experimentation and techniques like cross-validation.</p>
<p>An alternative approach is to assign weights to the neighbors, so that the closest neighbors will influence the prediction the most and neighbors further away will have much less influence on the prediction.</p>
</section>
</section>
<section id="illustrative-examples-with-travel-agency-data">
<h3>Illustrative Examples with Travel Agency Data<a class="headerlink" href="#illustrative-examples-with-travel-agency-data" title="Permalink to this heading">#</a></h3>
<p>Let’s see how KNN can be applied to Ada’s travel agency data:</p>
<p>Predicting Customer Satisfaction (Classification): Imagine Ada wants to predict whether a new customer will be satisfied with their trip based on their age, previous purchases, travel month, and other characteristics. KNN can be used to classify a new customer’s rating based on the rating levels of similar customers in the historical data.</p>
<p>Predicting Trip Costs (Regression): Ada could also use KNN to predict the cost of a new trip based on factors like destination, duration, number of guests, and travel month. KNN would find similar trips in the past and average their costs to provide an estimate for the new trip.</p>
</section>
</section>
<section id="detour-preprocessing-preparing-your-data-for-supervised-learning">
<h2>Detour: Preprocessing - Preparing Your Data for Supervised Learning<a class="headerlink" href="#detour-preprocessing-preparing-your-data-for-supervised-learning" title="Permalink to this heading">#</a></h2>
<p>Before feeding our data to a supervised learning algorithm, it’s often (read: almost always) crucial to preprocess it. Preprocessing involves transforming the data into a format that is more suitable for the algorithm, improving its performance, and ensuring reliable results. Here are two common preprocessing techniques: feature scaling and one-hot encoding.</p>
<section id="feature-scaling-leveling-the-playing-field">
<h3>Feature Scaling: Leveling the Playing Field<a class="headerlink" href="#feature-scaling-leveling-the-playing-field" title="Permalink to this heading">#</a></h3>
<p>Many machine learning algorithms, including KNN, are sensitive to the scale of the features (variables). If one feature has a much larger range of values than others, it can dominate the distance calculations and unfairly influence the model. Feature scaling helps to address this issue by bringing all features to a similar scale.</p>
<p>Common Scaling Techniques:</p>
<ul class="simple">
<li><p>Standardization (Z-score Normalization): Transforms the data to have zero mean and unit variance. Each feature’s values are adjusted by subtracting the mean and dividing by the standard deviation.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">scaled_value</span> <span class="pre">=</span> <span class="pre">(original_value</span> <span class="pre">-</span> <span class="pre">mean)</span> <span class="pre">/</span> <span class="pre">standard_deviation</span></code></p>
<ul class="simple">
<li><p>Min-Max Scaling (Normalization): Scales the data to a specific range, usually between 0 and 1. Each feature’s values are adjusted by subtracting the minimum and dividing by the range (maximum - minimum).</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">scaled_value</span> <span class="pre">=</span> <span class="pre">(original_value</span> <span class="pre">-</span> <span class="pre">min)</span> <span class="pre">/</span> <span class="pre">(max</span> <span class="pre">-</span> <span class="pre">min)</span></code></p>
<p>Choosing a Scaling Technique:</p>
<ul class="simple">
<li><p>Standardization is less sensitive to outliers but can result in negative values.</p></li>
<li><p>Min-Max scaling preserves the original distribution but is more sensitive to outliers.</p></li>
<li><p>The choice often depends on the specific algorithm and the characteristics of your data.</p></li>
</ul>
</section>
<section id="one-hot-encoding-representing-categorical-variables">
<h3>One-Hot Encoding: Representing Categorical Variables<a class="headerlink" href="#one-hot-encoding-representing-categorical-variables" title="Permalink to this heading">#</a></h3>
<p>Many machine learning algorithms require numerical input. If your data contains categorical variables (like destination type, travel month, etc.), you need to convert them into a numerical format. One-hot encoding is a common technique for this.</p>
<p>How One-Hot Encoding Works:</p>
<ol class="arabic simple">
<li><p>Create Dummy Variables: For each unique category in a categorical variable, a new binary (0/1) variable is created.</p></li>
<li><p>Assign Values: If a data point belongs to a particular category, the corresponding dummy variable is set to 1; otherwise, it’s set to 0.</p></li>
</ol>
<p>Example:
Let’s say you have a package_type variable with categories ‘adventure’, ‘relaxation’, and ‘cultural’. One-hot encoding would create three new variables:</p>
<ul class="simple">
<li><p>package_type_adventure: 1 if the package type is adventure, 0 otherwise.</p></li>
<li><p>package_type_relaxation: 1 if the package type is relaxation, 0 otherwise.</p></li>
<li><p>package_type_cultural: 1 if the package type is cultural, 0 otherwise.</p></li>
</ul>
<p>Important Considerations:</p>
<ul class="simple">
<li><p>Dummy Variable Trap: When using one-hot encoding, you should usually drop one of the dummy variables for each categorical feature to avoid multicollinearity (redundancy) issues. Using the package type example above, if you know the package type is not adventure and not relaxation, then it has to be cultural.</p></li>
<li><p>Alternatives: Other encoding techniques, like label encoding or ordinal encoding, might be suitable depending on the nature of your categorical variables.</p></li>
</ul>
<p>In summary, preprocessing is an essential step in preparing your data for supervised learning. Feature scaling ensures that all features contribute equally to the model, while one-hot encoding allows you to incorporate categorical variables. By carefully preprocessing your data, you can improve the performance and reliability of your machine learning models.</p>
</section>
</section>
<section id="decision-trees-branching-paths-to-predictions">
<h2>Decision Trees: Branching Paths to Predictions<a class="headerlink" href="#decision-trees-branching-paths-to-predictions" title="Permalink to this heading">#</a></h2>
<p>Decision trees are a popular supervised learning algorithm known for their interpretability and ability to handle both classification and regression tasks. They visually resemble a tree, with branches representing decision rules and leaves representing outcomes.</p>
<p>Building a Decision Tree
The process of building a decision tree involves recursively splitting the data based on features to create increasingly homogeneous subsets. The goal is to find the features and splitting points that best separate the data into groups with similar target values. Common algorithms for building decision trees include ID3, C4.5, and CART.</p>
<p>Key Concepts:</p>
<ul class="simple">
<li><p>Root Node: The topmost node, representing the entire dataset.</p></li>
<li><p>Internal Nodes: Nodes that represent decision points based on features.</p></li>
<li><p>Branches: Connections between nodes, representing the outcome of a decision.</p></li>
<li><p>Leaf Nodes (Terminal Nodes): Nodes that represent the final predictions (class labels for classification, values for regression).</p></li>
</ul>
<p>Example:
Imagine a decision tree for predicting customer satisfaction. The root node might split the data based on age (e.g., “age &lt; 30?”). If yes, the left branch might lead to a node that splits based on travel month, while the right branch might lead to a node that splits based on previous purchases. This process continues until the leaf nodes are reached, which would contain the final predictions (“satisfied” or “not satisfied”) or some number representing satisfaction level.</p>
<p>Advantages of Decision Trees</p>
<ul class="simple">
<li><p>Interpretability: Decision trees are easy to understand and visualize, making them ideal for explaining predictions to stakeholders.</p></li>
<li><p>Handling Non-linearity: They can capture non-linear relationships between features and the target variable.</p></li>
<li><p>Feature Importance: Decision trees can provide insights into which features are most important for making predictions.</p></li>
<li><p>No Feature Scaling: Decision trees are not sensitive to the scale of the features, so feature scaling is often not required.</p></li>
</ul>
<p>Limitations of Decision Trees</p>
<ul class="simple">
<li><p>Overfitting: Decision trees can be prone to overfitting, especially if they are deep (many levels). Techniques like pruning and setting limits on tree depth can help mitigate this.</p></li>
<li><p>Instability: Small changes in the data can lead to different tree structures, making them somewhat unstable. Ensemble methods (like random forests) can help address this.</p></li>
<li><p>Bias Towards Categorical Features with Many Categories: Decision trees can be biased towards categorical features with many categories, as they tend to create splits with high information gain for such features.</p></li>
</ul>
</section>
<section id="linear-regression-capturing-linear-relationships">
<h2>Linear Regression: Capturing Linear Relationships<a class="headerlink" href="#linear-regression-capturing-linear-relationships" title="Permalink to this heading">#</a></h2>
<p>Linear regression is a fundamental supervised learning algorithm used for predicting a continuous target variable based on a linear relationship with one or more predictor variables (features). That is to say it is a regression technique, as the name suggests. It’s a cornerstone of statistical modeling and machine learning, providing a simple yet powerful way to capture and quantify relationships between variables.</p>
<section id="the-equation-of-a-line">
<h3>The Equation of a Line:<a class="headerlink" href="#the-equation-of-a-line" title="Permalink to this heading">#</a></h3>
<p>At its core, linear regression seeks to find the best-fitting line that represents the relationship between the predictor variable(s) and the target variable. The equation of a line is typically represented as:
<code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">mx</span> <span class="pre">+</span> <span class="pre">b</span></code>
where:</p>
<ul class="simple">
<li><p>y is the target variable (the value we want to predict).</p></li>
<li><p>x is the predictor variable (the input feature).</p></li>
<li><p>m is the slope of the line (representing the change in y for a unit change in x).</p></li>
<li><p>b is the y-intercept (the value of y when x is 0).
In the context of our travel agency, y could be the cost of a trip, and x could be the stay_length. The slope m would then represent how much the cost increases per day of stay, and the intercept b would represent the base cost of the trip (even with a 0-day stay, which might include fixed costs like booking fees).</p></li>
</ul>
</section>
<section id="finding-the-best-fitting-line-minimizing-the-error">
<h3>Finding the Best-Fitting Line: Minimizing the Error<a class="headerlink" href="#finding-the-best-fitting-line-minimizing-the-error" title="Permalink to this heading">#</a></h3>
<p>Linear regression aims to find the values of m and c that minimize the difference between the predicted values (y) and the actual values in the training data. This difference is typically measured using the Mean Squared Error (MSE) or the Root Mean Squared Error (RMSE). The most common method for finding the best-fitting line is the least squares method, which minimizes the sum of the squared errors.</p>
</section>
<section id="interpreting-the-coefficients">
<h3>Interpreting the Coefficients<a class="headerlink" href="#interpreting-the-coefficients" title="Permalink to this heading">#</a></h3>
<p>Once the best-fitting line is found, the coefficients (m and b) provide valuable insights into the relationship between the variables:</p>
<ul class="simple">
<li><p>Slope (m): Indicates the direction and magnitude of the relationship. A positive slope means that as x increases, y also tends to increase. A negative slope means that as x increases, y tends to decrease. The magnitude of the slope tells you how much y changes for a one-unit change in x.</p></li>
<li><p>Intercept (b): Represents the value of y when x is 0. It can be interpreted as the baseline value of the target variable.</p></li>
</ul>
</section>
</section>
<section id="logistic-regression-predicting-probabilities">
<h2>Logistic Regression: Predicting Probabilities<a class="headerlink" href="#logistic-regression-predicting-probabilities" title="Permalink to this heading">#</a></h2>
<p>Logistic regression is a powerful supervised learning algorithm used for binary classification tasks, where the goal is to predict the probability of a data point belonging to a particular category. Unlike linear regression, which predicts a continuous output, logistic regression predicts a categorical output (typically 0 or 1). So in this case, the name can be misleading.</p>
<section id="the-sigmoid-function-mapping-values-to-probabilities">
<h3>The Sigmoid Function: Mapping Values to Probabilities<a class="headerlink" href="#the-sigmoid-function-mapping-values-to-probabilities" title="Permalink to this heading">#</a></h3>
<p>The key to logistic regression is the sigmoid function, which takes any input value (positive or negative) and maps it to a value between 0 and 1. This allows us to interpret the output of the logistic regression model as a probability.</p>
<p>The sigmoid function is defined as:
<code class="docutils literal notranslate"><span class="pre">sigmoid(z)</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(-z))</span></code>
where z is a linear combination of the input features and their weights, similar to the linear equation in linear regression.</p>
</section>
<section id="logistic-regression-for-binary-classification">
<h3>Logistic Regression for Binary Classification<a class="headerlink" href="#logistic-regression-for-binary-classification" title="Permalink to this heading">#</a></h3>
<p>In logistic regression, the sigmoid function is applied to the output of a linear equation to produce a probability. This probability is then used to make a classification decision. A threshold determines whether an instance is considered positive or not. While it can be any number between 0 and 1, let’s assume a threshold of 0.5 is used here:</p>
<ul class="simple">
<li><p>If the predicted probability is greater than or equal to 0.5, the data point is classified as 1 (e.g., “customer will book”).</p></li>
<li><p>If the predicted probability is less than 0.5, the data point is classified as 0 (e.g., “customer will not book”).</p></li>
</ul>
</section>
<section id="interpreting-coefficients">
<h3>Interpreting Coefficients<a class="headerlink" href="#interpreting-coefficients" title="Permalink to this heading">#</a></h3>
<p>The coefficients in a logistic regression model can be interpreted in terms of odds ratios. An odds ratio represents the change in the odds of the outcome (belonging to class 1) for a one-unit change in the predictor variable. Odds ratios greater than 1 indicate a positive association, while odds ratios less than 1 indicate a negative association.</p>
</section>
</section>
<section id="conclusion-the-power-of-predictive-modeling">
<h2>Conclusion: The Power of Predictive Modeling<a class="headerlink" href="#conclusion-the-power-of-predictive-modeling" title="Permalink to this heading">#</a></h2>
<p>In this chapter, we’ve ventured into the world of supervised learning, exploring powerful algorithms that enable us to predict outcomes and make informed decisions based on data. We discussed the difference between classification and regression and saw some techniques, like KNN and Decision Trees that can do both, and others, like Linear Regression and Logistic Regression, that are specific for either regression or classification.</p>
<p>As we’ve seen, each algorithm comes with its own set of assumptions and limitations. KNN relies on the notion of similarity, requiring careful consideration of distance metrics and the optimal number of neighbors. Linear regression assumes a linear relationship between variables, and logistic regression is specifically designed for binary classification tasks. Understanding these nuances is crucial for selecting the right tool for the job and interpreting the results accurately.</p>
<p>But the power of predictive modeling extends beyond mere technical proficiency. As we build AI systems that increasingly influence our lives, it’s essential to consider the ethical implications of our work. We must strive to create models that are fair, unbiased, and transparent, ensuring that they are used responsibly and do not perpetuate harmful societal biases.</p>
<p>The journey of supervised learning doesn’t end here. In the next chapter, we’ll look at model evaluation. What distinguishes good models from bad ones? How can we assess their performance, diagnose their weaknesses, and refine them for optimal accuracy and reliability? Get ready to unlock the secrets of model evaluation.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_3/chapter3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 3: Charting the unknown: stocking your explorer’s toolkit</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_5/chapter5.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 5: What makes good models good and bad models bad?</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-intuition-to-algorithms">From Intuition to Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-basic-rules-capturing-relationships">Beyond Basic Rules: Capturing Relationships</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-vs-regression-predicting-categories-or-values">Classification vs. Regression: Predicting Categories or Values</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-predicting-categories">Classification: Predicting Categories</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-in-the-travel-agency-context">Examples in the Travel Agency Context:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-predicting-values">Regression: Predicting Values</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Examples in the Travel Agency Context:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-right-task">Choosing the Right Task</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-knn-intuition-and-implementation">K-Nearest Neighbors (KNN): Intuition and Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-similarity-distance-metrics">Defining Similarity: Distance Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-knn-algorithm-finding-the-neighbors">The KNN Algorithm: Finding the Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-value-of-k">Choosing the Value of k</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrative-examples-with-travel-agency-data">Illustrative Examples with Travel Agency Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#detour-preprocessing-preparing-your-data-for-supervised-learning">Detour: Preprocessing - Preparing Your Data for Supervised Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling-leveling-the-playing-field">Feature Scaling: Leveling the Playing Field</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-representing-categorical-variables">One-Hot Encoding: Representing Categorical Variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-trees-branching-paths-to-predictions">Decision Trees: Branching Paths to Predictions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-capturing-linear-relationships">Linear Regression: Capturing Linear Relationships</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-equation-of-a-line">The Equation of a Line:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-best-fitting-line-minimizing-the-error">Finding the Best-Fitting Line: Minimizing the Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-coefficients">Interpreting the Coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-predicting-probabilities">Logistic Regression: Predicting Probabilities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-function-mapping-values-to-probabilities">The Sigmoid Function: Mapping Values to Probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-for-binary-classification">Logistic Regression for Binary Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-coefficients">Interpreting Coefficients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-the-power-of-predictive-modeling">Conclusion: The Power of Predictive Modeling</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chidi Nweke, Aimée Backiel, Daan Nijs, and Kenric Borgelioen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>