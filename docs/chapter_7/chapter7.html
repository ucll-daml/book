

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 7: What if we let AI find patterns on its own? &#8212; Data Analytics and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_7/chapter7';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Conclusion" href="../conclusion/conclusion.html" />
    <link rel="prev" title="Chapter 6: Is it really AI if I’m doing all the thinking?" href="../chapter_6/chapter6.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/Logo_UCLL_ENG_RGB.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/Logo_UCLL_ENG_RGB.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_1/chapter1.html">Chapter 1: What makes data science, machines learn, and intelligence artificial?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_2/chapter2.html">Chapter 2: Bringing the right equipment to start your data adventure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_3/chapter3.html">Chapter 3: Charting the unknown: stocking your explorer’s toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_4/chapter4.html">Chapter 4: AI isn’t magic, but it’s okay if it feels like it</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_5/chapter5.html">Chapter 5: What makes good models good and bad models bad?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_6/chapter6.html">Chapter 6: Is it really AI if I’m doing all the thinking?</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 7: What if we let AI find patterns on its own?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../conclusion/conclusion.html">Conclusion</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ucll-daml/book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ucll-daml/book/issues/new?title=Issue%20on%20page%20%2Fchapter_7/chapter7.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_7/chapter7.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 7: What if we let AI find patterns on its own?</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-beyond-labels">Introduction: Beyond Labels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-grouping-similar-data-points">Clustering: Grouping Similar Data Points</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-algorithms">Clustering Algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-qualitative-nature-of-clustering">The Qualitative Nature of Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-simplifying-data">Dimensionality Reduction: Simplifying Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-curse-of-dimensionality">The Curse of Dimensionality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-sne-t-distributed-stochastic-neighbor-embedding">t-SNE (t-Distributed Stochastic Neighbor Embedding)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-learning-through-interaction">Reinforcement Learning: Learning Through Interaction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-agent-environment-loop">The Agent-Environment Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rewards-and-penalties">Rewards and Penalties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-reinforcement-learning">Applications of Reinforcement Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-unleashing-the-power-of-unlabeled-data">Conclusion: Unleashing the Power of Unlabeled Data</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-7-what-if-we-let-ai-find-patterns-on-its-own">
<h1>Chapter 7: What if we let AI find patterns on its own?<a class="headerlink" href="#chapter-7-what-if-we-let-ai-find-patterns-on-its-own" title="Permalink to this heading">#</a></h1>
<section id="introduction-beyond-labels">
<h2>Introduction: Beyond Labels<a class="headerlink" href="#introduction-beyond-labels" title="Permalink to this heading">#</a></h2>
<p>In our exploration of supervised learning, we’ve witnessed the power of algorithms that learn from labeled data, where each data point comes with a clear answer or target variable. We’ve seen how these algorithms can predict customer satisfaction, estimate trip costs, and classify booking decisions. But what happens when we venture into the realm of unlabeled data, where the answers are hidden, and the patterns are waiting to be discovered?</p>
<p>Unlabeled data presents a unique challenge. Without explicit target variables to guide the learning process, we can’t simply train a model to predict outcomes. Instead, we need algorithms that can explore the data on their own, uncover hidden structures, and reveal insights that might otherwise remain hidden. This is the essence of unsupervised learning.</p>
<p>Unsupervised learning algorithms are like explorers venturing into uncharted territory. They sift through the data, looking for similarities, differences, and patterns that can help us make sense of the information. They don’t rely on predefined labels or instructions; they learn from the data itself, revealing its inherent structure and organization.</p>
<p>The applications of unsupervised learning are vast and varied:</p>
<ul class="simple">
<li><p>Clustering: Grouping similar data points together, allowing us to identify customer segments, discover patterns in travel behavior, or categorize destinations based on shared characteristics.</p></li>
<li><p>Dimensionality Reduction: Simplifying complex data by reducing the number of variables while preserving essential information. This can be invaluable for data visualization, feature extraction, and noise reduction.</p></li>
<li><p>Anomaly Detection: Identifying unusual or unexpected data points that deviate from the norm, which can be useful for detecting fraud, identifying outliers, or flagging potential problems.</p></li>
</ul>
<p>In this chapter, we’ll embark on a journey into the world of unsupervised learning, exploring its power and potential to unlock the secrets hidden within unlabeled data. We’ll delve into clustering algorithms, dimensionality reduction techniques, and other unsupervised learning approaches, discovering how they can help us gain valuable insights and make informed decisions, even when the answers aren’t readily apparent.</p>
</section>
<section id="clustering-grouping-similar-data-points">
<h2>Clustering: Grouping Similar Data Points<a class="headerlink" href="#clustering-grouping-similar-data-points" title="Permalink to this heading">#</a></h2>
<p>Clustering is a cornerstone of unsupervised learning, enabling us to discover hidden structures in data by grouping similar data points together. Imagine wanting to segment your travel agency’s customers into distinct groups based on their travel preferences, demographics, or purchase history. Clustering algorithms can help you achieve this without needing predefined labels or categories.</p>
<p>Similarity and Distance Metrics
Just like in K-Nearest Neighbors (KNN), the concept of similarity plays a crucial role in clustering. We use distance metrics (such as Euclidean distance, Manhattan distance, or cosine similarity) to quantify how similar or dissimilar data points are. Points that are closer together in the feature space are considered more similar.</p>
<section id="clustering-algorithms">
<h3>Clustering Algorithms<a class="headerlink" href="#clustering-algorithms" title="Permalink to this heading">#</a></h3>
<p>Clustering algorithms employ various strategies to group data points based on similarity. Here are some prominent examples:</p>
<ul class="simple">
<li><p>K-means Clustering: K-means is a popular and intuitive clustering algorithm. It requires you to specify the desired number of clusters (K) beforehand. The algorithm then works iteratively:</p></li>
</ul>
<ol class="arabic simple">
<li><p>Initialization: K initial cluster centers (centroids) are randomly chosen.</p></li>
<li><p>Assignment: Each data point is assigned to the cluster whose centroid is closest to it.</p></li>
<li><p>Update: The centroids of each cluster are updated based on the mean of the data points assigned to that cluster.</p></li>
<li><p>Iteration: Steps 2 and 3 are repeated until the centroids no longer change significantly or a maximum number of iterations is reached.</p></li>
</ol>
<ul class="simple">
<li><p>Advantages:</p></li>
<li><p>Simple to understand and implement.</p></li>
<li><p>Relatively efficient, especially for large datasets.</p></li>
<li><p>Disadvantages:</p></li>
<li><p>Sensitive to the initial placement of centroids. Different initializations can lead to different clustering results.</p></li>
<li><p>Struggles with non-spherical clusters or clusters of varying sizes and densities.</p></li>
<li><p>Requires specifying the number of clusters (K) beforehand, which might not always be known.</p></li>
<li><p>Determining the Optimal Number of Clusters:</p></li>
<li><p>Elbow Method: Plot the within-cluster sum of squares (WCSS) against the number of clusters (K). The “elbow” point in the plot, where the WCSS starts to decrease less rapidly, can suggest a good value for K.</p></li>
<li><p>Silhouette Analysis: Calculate the silhouette score for different values of K. The silhouette score measures how similar a data point is to its own cluster compared to other clusters. Higher silhouette scores indicate better-defined clusters.</p></li>
<li><p>K-medoid Clustering works in much the same way, but instead of representing each cluster by the mean, an actual instance in the cluster is used to represent the cluster.</p></li>
</ul>
<p>Clustering When K is Unknown:</p>
<ul class="simple">
<li><p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN identifies clusters based on the density of data points. It can discover clusters of arbitrary shapes and handle noise (outliers) effectively.</p></li>
<li><p>Hierarchical Clustering: Hierarchical clustering builds a hierarchy of clusters, represented as a dendrogram (tree-like diagram).</p></li>
<li><p>Agglomerative Clustering (bottom-up): Starts with each data point as a separate cluster and iteratively merges the closest clusters until a single cluster remains.</p></li>
<li><p>Divisive Clustering (top-down): Starts with all data points in a single cluster and recursively divides the cluster into smaller clusters until each data point is in its own cluster.</p></li>
</ul>
</section>
<section id="the-qualitative-nature-of-clustering">
<h3>The Qualitative Nature of Clustering<a class="headerlink" href="#the-qualitative-nature-of-clustering" title="Permalink to this heading">#</a></h3>
<p>While clustering algorithms provide valuable insights into the structure of data, interpreting the results often requires a qualitative approach. Visualizing the clusters, examining the characteristics of data points within each cluster, and using domain expertise to understand the meaning of the clusters are crucial steps in extracting meaningful insights.</p>
<p>Clustering is a powerful tool for exploratory data analysis, customer segmentation, pattern recognition, and anomaly detection. By understanding the different clustering algorithms and their strengths and weaknesses, you can effectively leverage this technique to uncover hidden structures and gain a deeper understanding of your data.</p>
</section>
</section>
<section id="dimensionality-reduction-simplifying-data">
<h2>Dimensionality Reduction: Simplifying Data<a class="headerlink" href="#dimensionality-reduction-simplifying-data" title="Permalink to this heading">#</a></h2>
<p>As we venture deeper into the world of data analysis, we often encounter datasets with a large number of features or variables. While more information can be beneficial, high-dimensional data presents unique challenges, often referred to as the “curse of dimensionality.”</p>
<section id="the-curse-of-dimensionality">
<h3>The Curse of Dimensionality<a class="headerlink" href="#the-curse-of-dimensionality" title="Permalink to this heading">#</a></h3>
<p>In high-dimensional spaces, data points become increasingly sparse, making it difficult to identify patterns and relationships. Distances between points become less meaningful, and traditional algorithms may struggle to perform effectively. This can lead to increased computational cost, overfitting, and reduced model interpretability.</p>
<p>Dimensionality Reduction to the Rescue</p>
<p>Dimensionality reduction techniques aim to address these challenges by reducing the number of features while preserving essential information. This simplification can lead to:</p>
<ul class="simple">
<li><p>Improved model performance</p></li>
<li><p>Faster training times</p></li>
<li><p>Reduced storage requirements</p></li>
<li><p>Enhanced data visualization</p></li>
<li><p>Better insights and interpretability</p></li>
</ul>
</section>
<section id="principal-component-analysis-pca">
<h3>Principal Component Analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Permalink to this heading">#</a></h3>
<p>PCA is a widely used linear dimensionality reduction technique. It identifies the principal components, which are new variables that are linear combinations of the original features. These principal components are orthogonal (uncorrelated) and ordered by the amount of variance they explain in the data.</p>
<p>By selecting a subset of the top principal components that capture most of the variance, we can reduce the dimensionality of the data while retaining the most important information. This is achieved by projecting the data onto the lower-dimensional subspace spanned by the selected principal components.</p>
</section>
<section id="t-sne-t-distributed-stochastic-neighbor-embedding">
<h3>t-SNE (t-Distributed Stochastic Neighbor Embedding)<a class="headerlink" href="#t-sne-t-distributed-stochastic-neighbor-embedding" title="Permalink to this heading">#</a></h3>
<p>t-SNE is a non-linear dimensionality reduction technique that is particularly well-suited for visualizing high-dimensional data. It focuses on preserving the local neighborhood relationships between data points, ensuring that points that are close together in the original high-dimensional space remain close together in the lower-dimensional representation. This makes t-SNE effective for revealing clusters and patterns in data that might not be apparent in the original high-dimensional space.</p>
<p>Applications of Dimensionality Reduction:</p>
<ul class="simple">
<li><p>Data Visualization: Reducing data to two or three dimensions allows us to visualize it and gain insights that might not be apparent from the raw data.</p></li>
<li><p>Feature Extraction: Principal components can be used as new features for machine learning models, potentially improving performance and interpretability.</p></li>
<li><p>Noise Reduction: By focusing on the principal components that capture the most variance, dimensionality reduction can help filter out noise and irrelevant information.</p></li>
</ul>
<p>Dimensionality reduction is a valuable tool in the data scientist’s arsenal. By simplifying data while preserving essential information, it enables us to tackle the challenges of high-dimensional data and gain deeper insights into its underlying structure and patterns.</p>
</section>
</section>
<section id="reinforcement-learning-learning-through-interaction">
<h2>Reinforcement Learning: Learning Through Interaction<a class="headerlink" href="#reinforcement-learning-learning-through-interaction" title="Permalink to this heading">#</a></h2>
<p>Reinforcement learning (RL) represents a distinct paradigm within machine learning, where an agent learns by interacting with an environment and receiving feedback in the form of rewards or penalties. Unlike supervised learning, where the agent learns from labeled data, or unsupervised learning, where the agent explores unlabeled data, reinforcement learning focuses on learning through trial and error, much like how humans learn to ride a bike or play a game.</p>
<section id="the-agent-environment-loop">
<h3>The Agent-Environment Loop<a class="headerlink" href="#the-agent-environment-loop" title="Permalink to this heading">#</a></h3>
<p>The core of reinforcement learning is the interaction between an agent and its environment.</p>
<ol class="arabic simple">
<li><p>Observation: The agent observes the current state of the environment.</p></li>
<li><p>Action: Based on its observations, the agent chooses an action to take.</p></li>
<li><p>Reward/Penalty: The environment provides feedback to the agent in the form of a reward or penalty, indicating the desirability of the action.</p></li>
<li><p>State Transition: The environment transitions to a new state based on the agent’s action.</p></li>
<li><p>Learning: The agent learns from the reward/penalty and updates its strategy (policy) to maximize future rewards.</p></li>
</ol>
<p>This loop continues, with the agent refining its behavior over time through repeated interactions with the environment.</p>
</section>
<section id="rewards-and-penalties">
<h3>Rewards and Penalties<a class="headerlink" href="#rewards-and-penalties" title="Permalink to this heading">#</a></h3>
<p>Rewards and penalties are crucial for guiding the agent’s learning process.</p>
<ul class="simple">
<li><p>Positive rewards encourage the agent to repeat actions that lead to desirable outcomes.</p></li>
<li><p>Negative rewards (penalties) discourage actions that lead to undesirable outcomes.</p></li>
</ul>
<p>The agent’s goal is to learn a policy that maximizes the cumulative reward over time.</p>
</section>
<section id="applications-of-reinforcement-learning">
<h3>Applications of Reinforcement Learning<a class="headerlink" href="#applications-of-reinforcement-learning" title="Permalink to this heading">#</a></h3>
<p>Reinforcement learning has shown remarkable success in various domains:</p>
<ul class="simple">
<li><p>Game Playing: RL agents have achieved superhuman performance in games like Go, chess, and Atari video games.</p></li>
<li><p>Robotics: RL is used to train robots to perform complex tasks, such as grasping objects, navigating environments, and collaborating with humans.</p></li>
<li><p>Control Systems: RL can optimize control systems for various applications, including traffic light control, resource management, and personalized recommendations.</p></li>
</ul>
<p>Reinforcement learning offers a powerful framework for learning complex behaviors in interactive environments. While it presents unique challenges, such as designing appropriate reward functions and ensuring exploration, its potential to create intelligent agents that can adapt and learn in dynamic environments is vast.</p>
</section>
</section>
<section id="conclusion-unleashing-the-power-of-unlabeled-data">
<h2>Conclusion: Unleashing the Power of Unlabeled Data<a class="headerlink" href="#conclusion-unleashing-the-power-of-unlabeled-data" title="Permalink to this heading">#</a></h2>
<p>In this chapter, we’ve stepped outside the realm of labeled data and explored the fascinating world of unsupervised learning. We’ve seen how clustering algorithms like K-means, DBSCAN, and hierarchical clustering can group similar data points together, revealing hidden structures and patterns. We’ve also delved into dimensionality reduction techniques like PCA and t-SNE, which simplify data while preserving essential information, enabling better visualization and analysis.</p>
<p>As with any machine learning approach, understanding the assumptions and limitations of each unsupervised learning algorithm is crucial. K-means, for instance, assumes that clusters are spherical and equally sized, which might not always hold true. DBSCAN excels at finding clusters of arbitrary shapes but requires careful tuning of its parameters. Hierarchical clustering provides a visual representation of cluster relationships but can be computationally expensive for large datasets.</p>
<p>Despite these limitations, unsupervised learning offers immense potential for discovering hidden patterns and generating valuable insights. By exploring unlabeled data, we can uncover customer segments, identify anomalies, visualize complex datasets, and gain a deeper understanding of the underlying structure of information.</p>
<p>Unsupervised learning complements supervised learning, providing a valuable toolkit for exploring data when labels are scarce or unavailable. It allows us to ask different questions, discover new patterns, and gain a more comprehensive understanding of the world around us.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_7"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_6/chapter6.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 6: Is it really AI if I’m doing all the thinking?</p>
      </div>
    </a>
    <a class="right-next"
       href="../conclusion/conclusion.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Conclusion</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-beyond-labels">Introduction: Beyond Labels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-grouping-similar-data-points">Clustering: Grouping Similar Data Points</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-algorithms">Clustering Algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-qualitative-nature-of-clustering">The Qualitative Nature of Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-simplifying-data">Dimensionality Reduction: Simplifying Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-curse-of-dimensionality">The Curse of Dimensionality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-sne-t-distributed-stochastic-neighbor-embedding">t-SNE (t-Distributed Stochastic Neighbor Embedding)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-learning-through-interaction">Reinforcement Learning: Learning Through Interaction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-agent-environment-loop">The Agent-Environment Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rewards-and-penalties">Rewards and Penalties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-reinforcement-learning">Applications of Reinforcement Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-unleashing-the-power-of-unlabeled-data">Conclusion: Unleashing the Power of Unlabeled Data</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chidi Nweke, Aimée Backiel, Daan Nijs, and Kenric Borgelioen
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>