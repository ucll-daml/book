{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from jupytercards import display_flashcards\n",
    "from jupyterquiz import display_quiz\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: AI isn't magic, but it's okay if it feels like it\n",
    "\n",
    "We've journeyed through the landscape of our data, gaining valuable insights through exploration and visualization. Armed with this knowledge, we can make informed decisions about our travel agency's operations, marketing strategies, and customer service. But as programmers, we crave efficiency and automation. Can't we go a step further and teach machines to make these decisions for us?\n",
    "\n",
    "## From Intuition to Algorithms\n",
    "Imagine trying to automate decisions based on our data. We could write countless if-then-else statements to capture every possible scenario:\n",
    "* \"If the customer is young and traveling with friends, recommend adventurous destinations.\"\n",
    "* \"If the customer is older and prefers luxury, suggest a relaxing beach vacation.\"\n",
    "* ...\n",
    "\n",
    "But this quickly becomes overwhelming. What if we could extract these decision-making rules directly from the data itself? This is the essence of supervised learning.\n",
    "\n",
    "The core intuition behind supervised learning is simple: similar inputs should lead to similar outputs. A young adventurer booking a trip to the mountains is likely to have similar preferences to another young adventurer. But how do we define \"similar\"? This leads us to algorithms like K-Nearest Neighbors (KNN), which classify new data points based on their proximity to known data points.\n",
    "\n",
    "This is where the magic of AI starts to emerge. We're not explicitly programming the rules; we're allowing the algorithm to learn them from the data. Sometimes, the output we want is a simple yes/no (classification, like predicting whether a customer will book a trip). Other times, it's a continuous value (regression, like predicting the price a customer is willing to pay).\n",
    "\n",
    "### Beyond Basic Rules: Capturing Relationships\n",
    "\n",
    "But we can be even smarter. Instead of just looking at similar inputs, what if we could learn a more general relationship between the input and output? This is where linear regression comes in. Remember those correlations we explored? Linear regression is like their sophisticated cousin, finding the best-fitting line to capture the relationship between two variables. It's also related to the concept of variance, though the exact details of how these statistical concepts intertwine can remain a bit magical for now.\n",
    "\n",
    "In this chapter, we'll delve into these fundamental supervised learning algorithms, exploring how they work, how to train them, and how to evaluate their performance. We'll also learn about essential techniques like feature scaling, which ensures that all our variables are on a level playing field. Get ready to unlock the power of predictive modeling and witness how AI can transform data into actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification vs. Regression: Predicting Categories or Values\n",
    "\n",
    "In the realm of supervised learning, there are two primary types of tasks: classification and regression. The choice between them depends on the nature of the output you're trying to predict.   \n",
    "\n",
    "#### Classification: Predicting Categories\n",
    "Classification tasks involve predicting a categorical or discrete output. The goal is to assign a data point to one of several predefined categories or classes. Think of it as sorting things into different buckets.\n",
    "\n",
    "##### Examples in the Travel Agency Context:\n",
    "* Predicting whether a customer will book a trip (yes/no).   \n",
    "* Classifying customer reviews as positive, negative, or neutral.   \n",
    "* Predicting the package type a customer will choose (adventure, relaxation, or cultural).\n",
    "\n",
    "#### Regression: Predicting Values\n",
    "\n",
    "Regression tasks involve predicting a continuous or numerical output. The goal is to estimate a value for a given input. Think of it as drawing a line or curve that best fits the data.   \n",
    "\n",
    "##### Examples in the Travel Agency Context:\n",
    "* Predicting the cost of a trip.\n",
    "* Predicting the length of stay for a customer.\n",
    "* Predicting the number of guests for a booking.\n",
    "  \n",
    "#### Choosing the Right Task\n",
    "The choice between classification and regression depends on the specific question you're trying to answer, but essentially it's very straightforward:\n",
    "* If you want to predict a category or label, use classification.\n",
    "* If you want to predict a numerical value, use regression.\n",
    "\n",
    "Sometimes, you might need to transform your data or reframe your question to fit one of these tasks. For example, if you want to predict customer satisfaction on a scale of 1 to 10, you could treat it as a regression problem (predicting the numerical satisfaction score) or as a classification problem (grouping satisfaction scores into categories like \"low,\" \"medium,\" and \"high\").\n",
    "\n",
    "Understanding the distinction between classification and regression is crucial for selecting the appropriate algorithms and evaluation metrics for your supervised learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN): Intuition and Implementation\n",
    "\n",
    "Let's start with a common supervised learning technique that illustrates how this all works.\n",
    "\n",
    "The core idea behind K-Nearest Neighbors (KNN) is remarkably simple yet surprisingly powerful. It's based on the intuition that similar things tend to be located near each other. Think about your own experiences:\n",
    "\n",
    "* Birds of a feather flock together.\n",
    "* You're more likely to share similar interests with your close friends than with strangers.\n",
    "* Houses in the same neighborhood often have similar architectural styles and values.\n",
    "\n",
    "KNN applies this same principle to data. It assumes that data points with similar characteristics (features) will be located close to each other in the feature space. This allows us to make predictions about a new data point by looking at its \"nearest neighbors\" – the data points that are most similar to it.   \n",
    "\n",
    "#### Defining Similarity: Distance Metrics\n",
    "But how do we measure \"similarity\" between data points? KNN uses distance metrics to quantify the similarity between data points in the feature space. Some common distance metrics include:   \n",
    "* Euclidean Distance: The straight-line distance between two points (like the Pythagorean theorem).   \n",
    "* Manhattan Distance: The distance between two points measured along axes at right angles (like walking city blocks).   \n",
    "* Chebyshev distance: The maximum metric or L∞ metric - the maximum of the absolute differences between the corresponding coordinates of the two points for each variable.   \n",
    " \n",
    "The choice of distance metric can influence the performance of the KNN algorithm, and the best choice often depends on the specific characteristics of the data.   \n",
    "\n",
    "#### The KNN Algorithm: Finding the Neighbors\n",
    "The KNN algorithm itself is straightforward:\n",
    "\n",
    "1. Calculate Distances: Calculate the distance between the new data point and all other data points in the training set.   \n",
    "2. Find Nearest Neighbors: Identify the k data points that are closest (most similar) to the new data point. These are the \"k-nearest neighbors.\"   \n",
    "3. Make Predictions: KNN (and many supervised learning techniques) works for both classification and regression problems.\n",
    " * Classification: For classification tasks, assign the new data point to the class that is most frequent among its k-nearest neighbors (majority voting).   \n",
    " * Regression: For regression tasks, predict the value of the new data point by averaging the values of its k-nearest neighbors.   \n",
    "\n",
    "##### Choosing the Value of k\n",
    "The parameter k (the number of nearest neighbors to consider) is a crucial factor in the performance of the KNN algorithm. A small value of k can lead to overfitting (the model is too sensitive to noise in the data), while a large value of k can lead to underfitting (the model is too simplistic and misses important patterns). The optimal value of k often needs to be determined through experimentation and techniques like cross-validation.\n",
    "\n",
    "An alternative approach is to assign weights to the neighbors, so that the closest neighbors will influence the prediction the most and neighbors further away will have much less influence on the prediction.\n",
    "\n",
    "#### Illustrative Examples with Travel Agency Data\n",
    "Let's see how KNN can be applied to Ada's travel agency data:\n",
    "\n",
    "Predicting Customer Satisfaction (Classification): Imagine Ada wants to predict whether a new customer will be satisfied with their trip based on their age, previous purchases, travel month, and other characteristics. KNN can be used to classify a new customer's rating based on the rating levels of similar customers in the historical data.\n",
    "\n",
    "Predicting Trip Costs (Regression): Ada could also use KNN to predict the cost of a new trip based on factors like destination, duration, number of guests, and travel month. KNN would find similar trips in the past and average their costs to provide an estimate for the new trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Preprocessing - Preparing Your Data for Supervised Learning\n",
    "\n",
    "Before feeding our data to a supervised learning algorithm, it's often (read: almost always) crucial to preprocess it. Preprocessing involves transforming the data into a format that is more suitable for the algorithm, improving its performance, and ensuring reliable results. Here are two common preprocessing techniques: feature scaling and one-hot encoding.   \n",
    "\n",
    "#### Feature Scaling: Leveling the Playing Field\n",
    "Many machine learning algorithms, including KNN, are sensitive to the scale of the features (variables). If one feature has a much larger range of values than others, it can dominate the distance calculations and unfairly influence the model. Feature scaling helps to address this issue by bringing all features to a similar scale.   \n",
    "\n",
    "Common Scaling Techniques:\n",
    "* Standardization (Z-score Normalization): Transforms the data to have zero mean and unit variance. Each feature's values are adjusted by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    " `scaled_value = (original_value - mean) / standard_deviation`\n",
    "\n",
    "* Min-Max Scaling (Normalization): Scales the data to a specific range, usually between 0 and 1. Each feature's values are adjusted by subtracting the minimum and dividing by the range (maximum - minimum).\n",
    "\n",
    " `scaled_value = (original_value - min) / (max - min)`\n",
    "  \n",
    "\n",
    "Choosing a Scaling Technique:\n",
    "* Standardization is less sensitive to outliers but can result in negative values.\n",
    "* Min-Max scaling preserves the original distribution but is more sensitive to outliers.\n",
    "* The choice often depends on the specific algorithm and the characteristics of your data.\n",
    "\n",
    "#### One-Hot Encoding: Representing Categorical Variables\n",
    "Many machine learning algorithms require numerical input. If your data contains categorical variables (like destination type, travel month, etc.), you need to convert them into a numerical format. One-hot encoding is a common technique for this.   \n",
    "\n",
    "How One-Hot Encoding Works:\n",
    "1. Create Dummy Variables: For each unique category in a categorical variable, a new binary (0/1) variable is created.\n",
    "2. Assign Values: If a data point belongs to a particular category, the corresponding dummy variable is set to 1; otherwise, it's set to 0.\n",
    "\n",
    "Example:\n",
    "Let's say you have a package_type variable with categories 'adventure', 'relaxation', and 'cultural'. One-hot encoding would create three new variables:\n",
    "* package_type_adventure: 1 if the package type is adventure, 0 otherwise.\n",
    "* package_type_relaxation: 1 if the package type is relaxation, 0 otherwise.\n",
    "* package_type_cultural: 1 if the package type is cultural, 0 otherwise.\n",
    "\n",
    "Important Considerations:\n",
    "* Dummy Variable Trap: When using one-hot encoding, you should usually drop one of the dummy variables for each categorical feature to avoid multicollinearity (redundancy) issues. Using the package type example above, if you know the package type is not adventure and not relaxation, then it has to be cultural. \n",
    "* Alternatives: Other encoding techniques, like label encoding or ordinal encoding, might be suitable depending on the nature of your categorical variables.\n",
    "\n",
    "In summary, preprocessing is an essential step in preparing your data for supervised learning. Feature scaling ensures that all features contribute equally to the model, while one-hot encoding allows you to incorporate categorical variables. By carefully preprocessing your data, you can improve the performance and reliability of your machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees: Branching Paths to Predictions\n",
    "Decision trees are a popular supervised learning algorithm known for their interpretability and ability to handle both classification and regression tasks. They visually resemble a tree, with branches representing decision rules and leaves representing outcomes.\n",
    "\n",
    "Building a Decision Tree\n",
    "The process of building a decision tree involves recursively splitting the data based on features to create increasingly homogeneous subsets. The goal is to find the features and splitting points that best separate the data into groups with similar target values. Common algorithms for building decision trees include ID3, C4.5, and CART.\n",
    "\n",
    "Key Concepts:\n",
    "* Root Node: The topmost node, representing the entire dataset.\n",
    "* Internal Nodes: Nodes that represent decision points based on features.\n",
    "* Branches: Connections between nodes, representing the outcome of a decision.\n",
    "* Leaf Nodes (Terminal Nodes): Nodes that represent the final predictions (class labels for classification, values for regression).\n",
    "\n",
    "Example:\n",
    "Imagine a decision tree for predicting customer satisfaction. The root node might split the data based on age (e.g., \"age < 30?\"). If yes, the left branch might lead to a node that splits based on travel month, while the right branch might lead to a node that splits based on previous purchases. This process continues until the leaf nodes are reached, which would contain the final predictions (\"satisfied\" or \"not satisfied\") or some number representing satisfaction level.\n",
    "\n",
    "Advantages of Decision Trees\n",
    "* Interpretability: Decision trees are easy to understand and visualize, making them ideal for explaining predictions to stakeholders.\n",
    "* Handling Non-linearity: They can capture non-linear relationships between features and the target variable.\n",
    "* Feature Importance: Decision trees can provide insights into which features are most important for making predictions.\n",
    "* No Feature Scaling: Decision trees are not sensitive to the scale of the features, so feature scaling is often not required.\n",
    "\n",
    "Limitations of Decision Trees\n",
    "* Overfitting: Decision trees can be prone to overfitting, especially if they are deep (many levels). Techniques like pruning and setting limits on tree depth can help mitigate this.\n",
    "* Instability: Small changes in the data can lead to different tree structures, making them somewhat unstable. Ensemble methods (like random forests) can help address this.\n",
    "* Bias Towards Categorical Features with Many Categories: Decision trees can be biased towards categorical features with many categories, as they tend to create splits with high information gain for such features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Capturing Linear Relationships\n",
    "Linear regression is a fundamental supervised learning algorithm used for predicting a continuous target variable based on a linear relationship with one or more predictor variables (features). That is to say it is a regression technique, as the name suggests. It's a cornerstone of statistical modeling and machine learning, providing a simple yet powerful way to capture and quantify relationships between variables.   \n",
    "\n",
    "#### The Equation of a Line:\n",
    "At its core, linear regression seeks to find the best-fitting line that represents the relationship between the predictor variable(s) and the target variable. The equation of a line is typically represented as:\n",
    "`y = mx + b`\n",
    "where:\n",
    "* y is the target variable (the value we want to predict).\n",
    "* x is the predictor variable (the input feature).\n",
    "* m is the slope of the line (representing the change in y for a unit change in x).\n",
    "* b is the y-intercept (the value of y when x is 0).\n",
    "In the context of our travel agency, y could be the cost of a trip, and x could be the stay_length. The slope m would then represent how much the cost increases per day of stay, and the intercept b would represent the base cost of the trip (even with a 0-day stay, which might include fixed costs like booking fees).\n",
    "\n",
    "#### Finding the Best-Fitting Line: Minimizing the Error\n",
    "Linear regression aims to find the values of m and c that minimize the difference between the predicted values (y) and the actual values in the training data. This difference is typically measured using the Mean Squared Error (MSE) or the Root Mean Squared Error (RMSE). The most common method for finding the best-fitting line is the least squares method, which minimizes the sum of the squared errors.\n",
    "\n",
    "#### Interpreting the Coefficients\n",
    "Once the best-fitting line is found, the coefficients (m and b) provide valuable insights into the relationship between the variables:\n",
    "* Slope (m): Indicates the direction and magnitude of the relationship. A positive slope means that as x increases, y also tends to increase. A negative slope means that as x increases, y tends to decrease. The magnitude of the slope tells you how much y changes for a one-unit change in x.\n",
    "* Intercept (b): Represents the value of y when x is 0. It can be interpreted as the baseline value of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Predicting Probabilities\n",
    "Logistic regression is a powerful supervised learning algorithm used for binary classification tasks, where the goal is to predict the probability of a data point belonging to a particular category. Unlike linear regression, which predicts a continuous output, logistic regression predicts a categorical output (typically 0 or 1). So in this case, the name can be misleading.\n",
    "\n",
    "#### The Sigmoid Function: Mapping Values to Probabilities\n",
    "The key to logistic regression is the sigmoid function, which takes any input value (positive or negative) and maps it to a value between 0 and 1. This allows us to interpret the output of the logistic regression model as a probability.\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "`sigmoid(z) = 1 / (1 + exp(-z))`\n",
    "where z is a linear combination of the input features and their weights, similar to the linear equation in linear regression.\n",
    "\n",
    "#### Logistic Regression for Binary Classification\n",
    "In logistic regression, the sigmoid function is applied to the output of a linear equation to produce a probability. This probability is then used to make a classification decision. A threshold determines whether an instance is considered positive or not. While it can be any number between 0 and 1, let's assume a threshold of 0.5 is used here:\n",
    "* If the predicted probability is greater than or equal to 0.5, the data point is classified as 1 (e.g., \"customer will book\").\n",
    "* If the predicted probability is less than 0.5, the data point is classified as 0 (e.g., \"customer will not book\").\n",
    "\n",
    "#### Interpreting Coefficients\n",
    "The coefficients in a logistic regression model can be interpreted in terms of odds ratios. An odds ratio represents the change in the odds of the outcome (belonging to class 1) for a one-unit change in the predictor variable. Odds ratios greater than 1 indicate a positive association, while odds ratios less than 1 indicate a negative association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: The Power of Predictive Modeling\n",
    "\n",
    "In this chapter, we've ventured into the world of supervised learning, exploring powerful algorithms that enable us to predict outcomes and make informed decisions based on data. We discussed the difference between classification and regression and saw some techniques, like KNN and Decision Trees that can do both, and others, like Linear Regression and Logistic Regression, that are specific for either regression or classification.\n",
    "\n",
    "As we've seen, each algorithm comes with its own set of assumptions and limitations. KNN relies on the notion of similarity, requiring careful consideration of distance metrics and the optimal number of neighbors. Linear regression assumes a linear relationship between variables, and logistic regression is specifically designed for binary classification tasks. Understanding these nuances is crucial for selecting the right tool for the job and interpreting the results accurately.\n",
    "\n",
    "But the power of predictive modeling extends beyond mere technical proficiency. As we build AI systems that increasingly influence our lives, it's essential to consider the ethical implications of our work. We must strive to create models that are fair, unbiased, and transparent, ensuring that they are used responsibly and do not perpetuate harmful societal biases.\n",
    "\n",
    "The journey of supervised learning doesn't end here. In the next chapter, we'll look at model evaluation. What distinguishes good models from bad ones? How can we assess their performance, diagnose their weaknesses, and refine them for optimal accuracy and reliability? Get ready to unlock the secrets of model evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
